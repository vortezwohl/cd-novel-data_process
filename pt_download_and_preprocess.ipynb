{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BASE_NAME = 'Lorena Scaglia Dataset'\n",
    "INPUT_FILE_NAME = f'processed_data/{BASE_NAME}.xlsx'\n",
    "SHEET_FILE_NAME_STAGE_1 = f'train_dataset/{BASE_NAME} Stage1.csv'\n",
    "SHEET_FILE_NAME_STAGE_2 = f'train_dataset/{BASE_NAME} Stage2.jsonl'\n",
    "SHEET_FILE_NAME_STAGE_3 = f'train_dataset/{BASE_NAME} Stage3.jsonl'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f667413bf8eb8566",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52fedc7aee5a482f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = pd.read_excel(INPUT_FILE_NAME)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec7a081d7b9d58a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = data.to_dict(orient='records')\n",
    "len_data = len(data)\n",
    "print(f'Len: {len(data)}')\n",
    "data[0].keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1696441491e38d22",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_jsonl(data_list: list, file_path: str):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data_list:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "843c9e2067a12c44",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 构建 SFT 数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b40f45fc903a07ef"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from any_llm import LLM\n",
    "from prompt4py import GeneralTemplate\n",
    "from vortezwohl.iter import sliding_window\n",
    "\n",
    "load_dotenv()\n",
    "claude = LLM(provider='new_api', model_name='gemini-2.5-pro')\n",
    "\n",
    "\n",
    "def chapter_chunking(text: str) -> list[str]:\n",
    "    lines = text.lstrip().splitlines(keepends=False)\n",
    "    chunks = []\n",
    "    for chunk_lines in sliding_window(lines, window_size=5, stride=1, fill_value=None):\n",
    "        if None not in chunk_lines:\n",
    "            chunks.append('\\n'.join(chunk_lines))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_analyse(origin: str, translation: str, polish: str):\n",
    "    prompt_template = GeneralTemplate()\n",
    "    prompt_template.role = '你是一个以**巴西**葡萄牙语为母语的小说编辑, 你擅长对葡萄牙语小说进行精修.'\n",
    "    prompt_template.context = {\n",
    "        '任务背景-大背景': '我们的翻译同事收到了一份原文稿件([原文稿]), 并对其进行了初步的葡萄牙语翻译([翻译稿]), 但从英文翻译到葡萄牙语的[翻译稿]还是有很多细节对葡萄牙语母语者而言并不流畅地道, 所以现在请你对[翻译稿]进行精修([精修稿]).',\n",
    "        '精修过程': '精修的目的通常是让文章具有更好流畅度、自然感与丰富的文体风格. 精修时主要对[翻译稿]进行精修, 有时也会直接参考[原文稿]进行\"更好的翻译\".',\n",
    "        '上下文的重要性': '通常情况下, 一句话的精修是与其上下文(特别是上文)高度相关的, 你需要密切关注这种联系.'\n",
    "    }\n",
    "    prompt_template.input = {\n",
    "        '原文稿': '{{origin}}',\n",
    "        '翻译稿': '{{translation}}',\n",
    "        '精修稿': '{{polish}}'\n",
    "    }\n",
    "    prompt_template.instruction = {\n",
    "        '具体任务': '摆在你眼前的有[原文稿][翻译稿]和[精修稿], 其中[精修稿]出自你手, 但你当初在精修的时候并没有充分解释你的精修过程, 现在我需要你对着[精修稿]重现你的精修思路, 也就是说, 我需要你通过讲解具体思路的方式, 讲解你是如何从[翻译稿]得到[精修稿]的(对于被修改的词汇/句式/语段, 你需要给出具体的修改理由; 对于没被修改的词汇/句式/语段, 你也需要说明为什么不需要修改; 你必须覆盖所有被修改的词汇/句式/语段, 同时尽可能覆盖更多的没被修改的词汇/句式/语段)',\n",
    "    }\n",
    "    prompt_template.output_format = '[COT] ...'\n",
    "    prompt_template.output_dtype = 'plaintext'\n",
    "    prompt_template.output_language = 'zh-cn'\n",
    "    prompt_template.constraint = [\n",
    "        '输出以 \"[COT]现在我将以一个小说编辑的身份, 详细讲解我是如何将这份翻译初稿精修成最终稿的. 我的目标是让文本不仅仅是“正确”的, 更是要让它在节奏、语感和文学性上都达到母语者的阅读标准. 对于语句 ...\" 开始',\n",
    "        '假设你正在进行精修的过程, 不要提到复盘等, 直接说明你的详细精修思路',\n",
    "        '不要输出 \"精修稿\", 你要模拟真正的精修过程, 所以 \"精修稿\" 是在精修思路后才形成的, 而不是思路完整前',\n",
    "        '请只输出精修的思路和过程, 不要输出任何精修后的完整文段', '请使用中文向我汇报', '请始终使用中文向我汇报']\n",
    "    return claude(prompt_template.render(origin=origin, translation=translation, polish=polish, markdown=True),\n",
    "                  trace=f'精修思维链数据生成-[{time.time()}]', temperature=.0, top_p=.0, seed=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3870fdbd5f295a8e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from threading import RLock\n",
    "from vortezwohl.concurrent import ThreadPool\n",
    "\n",
    "threads = ThreadPool(32)\n",
    "train_dataset = []\n",
    "train_dataset_lock = RLock()\n",
    "\n",
    "\n",
    "def context_chunk_analyse(origin: str, translation: str, polish: str):\n",
    "    if len(train_dataset) > len_data:\n",
    "        return\n",
    "    analysis = chunk_analyse(origin=origin, translation=translation, polish=polish).replace('\\n', '')\n",
    "    with train_dataset_lock:\n",
    "        train_dataset.append({\n",
    "            'origin': origin,\n",
    "            'translation': translation,\n",
    "            'polish': polish,\n",
    "            'analysis': analysis\n",
    "        })\n",
    "        if len(train_dataset) % 25 == 0:\n",
    "            pd.DataFrame(train_dataset).to_csv(SHEET_FILE_NAME_STAGE_1, index=False)\n",
    "\n",
    "\n",
    "for d_item in data:\n",
    "    threads.submit(context_chunk_analyse, origin=d_item['章节内容'], translation=d_item['机翻内容'],\n",
    "                   polish=d_item['翻译精修'])\n",
    "\n",
    "\n",
    "res = threads.wait_all()\n",
    "res[0].traceback, res[0].returns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "170a260a1cda5bfa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pd.DataFrame(train_dataset).to_csv(SHEET_FILE_NAME_STAGE_1, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f963957827300f05",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(SHEET_FILE_NAME_STAGE_1).to_dict(orient='records')\n",
    "data[0].keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5346a3f7a81f9be2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def format_prompt(origin: str, translation: str) -> str:\n",
    "    prompt_template = GeneralTemplate()\n",
    "    prompt_template.role = '你是一个以**巴西**葡萄牙语为母语的小说编辑, 你擅长对葡萄牙语小说进行精修.'\n",
    "    prompt_template.context = {\n",
    "        '任务背景': '我们的翻译同事收到了一份原文稿件([原文]), 并对其进行了初步的葡萄牙语翻译([译文]), 但从英文翻译到葡萄牙语的[译文]还是有很多细节对葡萄牙语母语者而言并不流畅地道, 所以现在需要你对[译文]进行精修.'\n",
    "    }\n",
    "    prompt_template.input = {\n",
    "        '原文': '{{origin}}',\n",
    "        '译文': '{{translation}}'\n",
    "    }\n",
    "    prompt_template.objective = '思考如何对[译文]进行精修, 仅输出思考决策过程细节.'\n",
    "    prompt_template.instruction = '摆在你眼前的有[原文]和[译文], 我需要你深入思考如何对[译文]进行精修.'\n",
    "    prompt_template.output_format = '<think> ... </think>'\n",
    "    prompt_template.output_dtype = 'plaintext'\n",
    "    prompt_template.constraint = '输出以 \"<think>\" 开始'\n",
    "    return prompt_template.render(origin=origin, translation=translation, markdown=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbba0a888f62684",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for d_item in data:\n",
    "    analysis = d_item['analysis']\n",
    "    if analysis.lstrip().startswith('[COT]'):\n",
    "        analysis = analysis.replace('[COT]', '', 1)\n",
    "    analysis = f'<think>{analysis}</think>'\n",
    "    dataset.append({\n",
    "        'contents': [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'parts': [{'text': format_prompt(origin=d_item['origin'], translation=d_item['translation'])}]\n",
    "            },\n",
    "            {\n",
    "                'role': 'model',\n",
    "                'parts': [{'text': analysis}]\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "\n",
    "save_jsonl(dataset, SHEET_FILE_NAME_STAGE_2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1543d15ede66959e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "oss_dataset = []\n",
    "\n",
    "for d_item in data:\n",
    "    analysis = d_item['analysis']\n",
    "    if analysis.lstrip().startswith('[COT]'):\n",
    "        analysis = analysis.replace('[COT]', '', 1)\n",
    "    analysis = f'<think>{analysis}</think>'\n",
    "    oss_dataset.append({'prompt': format_prompt(origin=d_item['origin'], translation=d_item['translation']), 'completion': analysis})\n",
    "\n",
    "save_jsonl(oss_dataset, SHEET_FILE_NAME_STAGE_3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58d37f63cfb31488",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
